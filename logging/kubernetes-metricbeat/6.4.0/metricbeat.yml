########################## Metricbeat Configuration ###########################

#==========================  Modules configuration ============================
metricbeat.modules:

#-----------------------------------------------
# Kubernetes metrics
#-----------------------------------------------
- module: kubernetes
  metricsets:
    - node
    - system
    - pod
    - container
    - volume
  period: 10s
  host: ${HOSTNAME}
  hosts: ["localhost:10255"]
  # If using Red Hat OpenShift remove the previous hosts entry and 
  # uncomment these settings:
  #hosts: ["https://${HOSTNAME}:10250"]
  #bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  #ssl.certificate_authorities:
    #- /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
  # Enriching parameters:
  add_metadata: true
  in_cluster: true
  
#================================ General ======================================

# The tags of the shipper are included in their own field with each
# transaction published. Tags make it easy to group servers by different
# logical properties.
tags: ${BEAT_TAGS}

#================================ Processors ===================================

# Processors are used to reduce the number of fields in the exported event or to
# enhance the event with external metadata. This section defines a list of
# processors that are applied one by one and the first one receives the initial
# event:
#
#   event -> filter1 -> event1 -> filter2 ->event2 ...
#
# The supported processors are drop_fields, drop_event, include_fields, and
# add_cloud_metadata.
#
# For example, you can use the following processors to keep the fields that
# contain CPU load percentages, but remove the fields that contain CPU ticks
# values:
#
#processors:
#- include_fields:
#    fields: ["cpu"]
#- drop_fields:
#    fields: ["cpu.user", "cpu.system"]
#
# The following example drops the events that have the HTTP response code 200:
#
#processors:
#- drop_event:
#    when:
#       equals:
#           http.code: 200
#
# The following example enriches each event with metadata from the cloud
# provider about the host machine. It works on EC2, GCE, DigitalOcean,
# Tencent Cloud, and Alibaba Cloud.
#
processors:
- add_cloud_metadata: ~
#
# The following example enriches each event with the machine's local time zone
# offset from UTC.
#
#processors:
#- add_locale:
#    format: offset
#

#================================ Outputs ======================================

# Configure what outputs to use when sending the data collected by the beat.
# Multiple outputs may be used.

#-------------------------- Elasticsearch output -------------------------------
output.elasticsearch:
  # Boolean flag to enable or disable the output module.
  #enabled: true

  # Array of hosts to connect to.
  # Scheme and port can be left out and will be set to the default (http and 9200)
  # In case you specify and additional path, the scheme is required: http://localhost:9200/path
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:9200
  hosts: ["${ES_HOST}:${ES_PORT}"]

  # Optional protocol and basic auth credentials.
  protocol: "${ES_PROT}"
  username: "${ES_USER}"
  password: "${ES_PASS}"

  # Dictionary of HTTP parameters to pass within the url with index operations.
  #parameters:
    #param1: value1
    #param2: value2

  # Number of workers per Elasticsearch host.
  #worker: 1

  # Optional index name. The default is "metricbeat" plus date
  # and generates [metricbeat-]YYYY.MM.DD keys.
  #index: "metricbeat-%{[beat.version]}-%{+yyyy.MM.dd}"
  index: "${ES_INDEX}"

  # Compression of the submitted logs: 0-9, higher = more CPU usage
  # See: https://www.elastic.co/guide/en/beats/filebeat/current/elasticsearch-output.html#_compression_level
  #
  # NOTE: This is not supported in AWS Elasticsearch
  compression_level: ${ES_COMPRESSION}

  # The maximum number of events to bulk in a single Elasticsearch bulk API index request.
  bulk_max_size: ${ES_BATCHSIZE}

  # Optional ingest node pipeline. By default no pipeline will be used.
  #pipeline: ""

  # Optional HTTP Path
  #path: "/elasticsearch"

  # Custom HTTP headers to add to each request
  #headers:
  #  X-My-Header: Contents of the header

  # Proxy server url
  #proxy_url: http://proxy:3128

  # The number of times a particular Elasticsearch index operation is attempted. If
  # the indexing operation doesn't succeed after this many retries, the events are
  # dropped. The default is 3.
  #max_retries: 3

  # The maximum number of events to bulk in a single Elasticsearch bulk API index request.
  # The default is 50.
  #bulk_max_size: 50

  # Configure http request timeout before failing an request to Elasticsearch.
  #timeout: 90

  # The number of seconds to wait for new events between two bulk API index requests.
  # If `bulk_max_size` is reached before this interval expires, addition bulk index
  # requests are made.
  #flush_interval: 1s

  # Use SSL settings for HTTPS. Default is true.
  #ssl.enabled: true

  # Configure SSL verification mode. If `none` is configured, all server hosts
  # and certificates will be accepted. In this mode, SSL based connections are
  # susceptible to man-in-the-middle attacks. Use only for testing. Default is
  # `full`.
  #ssl.verification_mode: full

  # List of supported/valid TLS versions. By default all TLS versions 1.0 up to
  # 1.2 are enabled.
  #ssl.supported_protocols: [TLSv1.0, TLSv1.1, TLSv1.2]

  # SSL configuration. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"

  # Optional passphrase for decrypting the Certificate Key.
  #ssl.key_passphrase: ''

  # Configure cipher suites to be used for SSL connections
  #ssl.cipher_suites: []

  # Configure curve types for ECDHE based cipher suites
  #ssl.curve_types: []

############################# Setup #########################################
# https://www.elastic.co/guide/en/beats/filebeat/6.2/configuration-template.html
# Configure setup for templates when index is changed by user.
# Multiple setup may be used.
setup:

  ### templates for setup
  template:
    # Name for template setup
    name: "${ES_TEMPLATE_NAME}"

    # Pattern for template setup
    pattern: "${ES_TEMPLATE_PATTERN}"

  ### Dashboard for setup
  # dashboards:
  #   # Pattern for the dashboard index
  #   index: "${DASH_INDEX}"

#================================ Logging ======================================
# There are three options for the log output: syslog, file, stderr.
# Under Windows systems, the log files are per default sent to the file output,
# under all other system per default to syslog.

# Sets log level. The default log level is info.
# Available log levels are: critical, error, warning, info, debug
logging.level: "${BEAT_LOGGING_LEVEL}"

# Enable debug output for selected components. To enable all selectors use ["*"]
# Other available selectors are "beat", "publish", "service"
# Multiple selectors can be chained.
#logging.selectors: ["*"]

# Send all logging output to syslog. The default is false.
#logging.to_syslog: true

# If enabled, metricbeat periodically logs its internal metrics that have changed
# in the last period. For each metric that changed, the delta from the value at
# the beginning of the period is logged. Also, the total values for
# all non-zero internal metrics are logged on shutdown. The default is true.
#logging.metrics.enabled: true

# The period after which to log the internal metrics. The default is 30s.
logging.metrics.period: "${BEAT_LOGGING_PERIOD}"

# # Logging to rotating files files. Set logging.to_files to false to disable logging to
# # files.
# logging.to_files: true
# logging.files:
#   # Configure the path where the logs are written. The default is the logs directory
#   # under the home path (the binary location).
#   path: /var/log/metricbeat

#   # The name of the files where the logs are written to.
#   name: metricbeat

#   # Configure log file size limit. If limit is reached, log file will be
#   # automatically rotated
#   rotateeverybytes: 10485760 # = 10MB

#   # Number of rotated log files to keep. Oldest files will be deleted first.
#   keepfiles: 10
